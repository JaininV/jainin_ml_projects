Building a data center in 2023 is very different from building one 10 years ago — or even five years ago. How they’re used and the workloads deployed within them are evolving rapidly, and it’s not enough to just build for what you need; you also have to anticipate what you’re going to need in the future, which isn’t always easy.

Data centers are feeling the pinch on many fronts; for some time, workloads were moving to the cloud and companies were looking to dispose of their data centers entirely. Then reality intruded, and now workloads are returning to on-prem data centers. Of course, there’s also the power concern, which is become all-encompassing as concerns about data center power consumption have risen to the forefront.

There are many other data center construction issues as well. Workloads are changing, and that means the hardware configuration is changing. The types of hardware coming from OEMs are also changing, as are the ways of managing and cooling that hardware.

All of this leads into the biggest item to keep in mind: planning.

There’s an old saying, or cliché, that if you fail to plan you better plan to fail. That sentiment best describes building a data center in 2023. The days of throwing up for walls and a raised floor and loading it up with hardware are over. Elaborate, complex, in-depth preparation before you even break ground is an absolute must.

“You have to be so much more careful so that you don't run into any hiccups of roadblocks or challenges with permitting challenges with land acquisition,” said Ashish Nadkarni, group vice president and general manager of IDC's worldwide infrastructure research organization. ”Do as much pre-planning as you possibly can, even to the extent of doing research studies. Preparation is far more important than the building.”

“The number one thing I would tell anybody looking to construct infrastructure today is do as much pre-planning and consultative research as you can,” said Bill Kleyman, a consultant for colocation provider Neuro and a DCK contributor. “You should even go so far as to spend money on things like site evaluations, environmental planning and mapping before any ink dries on any paper, because your time to market and your speed to market is so important today.”

Fortunately, much of a given data center’s design is prefabricated, notes Kleyman, and prefabrication and modular infrastructure are quite helpful — you could even get 70% to 80% of the infrastructure pre-built.

Overall, data centers are becoming increasingly modular, both in construction and operation. That’s because the computer equipment is not homogenous anymore. There are areas of extreme density for things like HPC and AI, and then there are less dense areas for general compute and storage. This requires different levels of power and cooling to accommodate the needs of the compute systems.

“Data centers tend to be a lot more organized than in the past,” said Nadkarni. “There's a lot of focus on cabling. There's a lot of focus on cooling, for hot cold zones, and airflow power distribution.”

That extends to service quality zones, which means different zones in the data center have different uptime and quality of service promises. The entire data center cannot be one fault zone or failure zone. You need to have multiple quality of service locations within the data center where you can maintain them independent of each other, he added.

“Data centers are moving beyond their old role as four walls wrapped around racks and servers. They now have to serve as rich connectivity ecosystems that empower data mobility to support next-gen applications like IoT, AI and VR, autonomous vehicles and more,” said Michael Roark, CEO of iM Critical, a data center and IT services company. “So, building in connectivity to clouds, platforms, partners and other key business sites is vital for helping customers tap into that rich and mission-critical IT fabric.”

Data center power consumption is part of most companies attempt to be good stewards through their environmental, social, and governance (ESG) initiatives. But it’s more than just environmental concern. There are real issues surrounding available power and data centers, especially in dense urban areas.

Estimates of worldwide data center power consumption put it at 1% to 2% of global power. Data center operators, whether they be hyperscalers like Google or enterprises, have come to the realization they can’t just build out data centers and assume power will be there.

The considerations go far beyond the power and cooling requirements, notes Chris Noble, CEO and co-founder of Cirrus Nexus, a cloud management platform. “Data centers, particularly hyper-scale data centers, need to assure that future resources will be available, which may include operating their own sources of energy and water. Ensuring uninterrupted operations will require data center operators to look at grid-scale energy storage,” he said.

Additionally, the materials to build new data centers and expand existing ones will need to be sustainable, Noble added. “The future data center will need to be a self-sufficient, true zero-carbon and a water-neutral operation with minimal impact on regional and local environments,” he said.

Finally, with cooling becoming such a major issue, the best refrigeration can come from nature. Nic Kilby, hosting operations engineer at UK software firm Zengenti, said the company took advantage of the cool, windy local weather to implement natural cooling technologies.

“Free cooling has been a highly-efficient and energy-saving option, using outside air to cool the data center when the temperature is cold enough. We’ve also recently added a second fan to increase the airflow drawn into the center from the outside, meaning the system will be able to run for longer on warmer days,” he said.

Data centers are getting bigger and more compute-dense than they were a few years ago. That’s because the computing workloads are getting bigger — AI and machine learning chief among them — but, in general, high-performance computing is gaining popularity alongside business and data analytics. All of these require a lot of computing power and a lot more cooling power as well.

“Data-intensive businesses are moving beyond big data into the realm of hyperscale data, which is exponentially greater. And that requires a reevaluation of data infrastructure,” said Chris Gladwin, CEO and co-founder of Ocient, a data warehousing software company.

“It’s not just the overall volume of data that technologists must plan for, but also the burgeoning data sets and workloads to be processed,” Gladwin said. “Some leading-edge IT organizations are now working with data sets that comprise billions and trillions of records. In 2023, we could even see data sets of a quadrillion rows in data-intensive industries such as adtech, telecommunications, and geospatial.”

Another thing to consider when building a data center: The nature of data is changing. There are both more data types and more complex data types with the lines continuing to blur between structured and semi-structured data, notes Gladwin.

“The need to analyze new and more complex data types, including semi-structured data, will gain strength in the years ahead, driven by digital transformation and global business requirements,” he said.

It’s not just AI and HPC that has data centers running 24/7. Data warehouses are becoming “always on” analytics environments. In the years ahead, the flow of data into and out of data warehouses will be not only faster, but continuous. That means a greater need for compute power and storage.

In addition to the greater demands for power and cooling, there are greater demands for networking as data sources and volumes increase, says Jay Anderson, CTO and chief engineer at FiberLight, fiber-optic networking provider.

“An issue of consideration for a modern data center is flexible and economical connectivity at high availability and reasonable costs for the customer,” he said. “Easy, affordable access to multiple providers and on-ramps is a necessity for businesses today to exchange data with hundreds of other major carriers and ISPs.”

One thing to ask yourself before building a data center is do you want or need it? Colocation has become an increasingly viable alternative to both the cloud and on premises data centers.

“I would evaluate whether you want to be in the business of building and operating a data center in the first place,” said Nadkari. “There are a lot of enterprises who basically just go to an Equinix or any of these data center operators and say, I'm going to just lease some floor space and I'm just going to operate my portion in your data center. I just don't want to be in the business of building and maintaining a data center.”

Given the increasing complexity of the data center, it’s becoming less and less of a do-it-yourself project. Do not hesitate to bring in a data center expert that specializes in constructing data centers.

Also, get over your fear of liquid cooling. Liquid cooling is 3000 times more efficient at heat removal than air cooling and is the only option for compute-dense scenarios like HPC and AI. However, some people hear “water cooling” and fret over leaks destroying their equipment.

The fact is, liquid cooling is a mature market and the technology has been around for a while. It’s been proven solid and reliable. “We are at a maturity point with liquid cooling where your environment can more than likely sustain an air-cooled environment alongside a liquid-cooled environment,” said Kleyman.